\begin{thebibliography}{10}

\bibitem{ahn2024language}
{\sc Ahn, J., Oh, Y., Im, H., Yoon, S., and Cho, S.}
\newblock Do multilingual language models think better in english?
\newblock {\em arXiv preprint arXiv:2308.01223\/} (2024).

\bibitem{belinkov2017neural}
{\sc Belinkov, Y., Durrani, N., Dalvi, F., Sajjad, H., and Glass, J.}
\newblock What do neural machine translation models learn about morphology?
\newblock {\em arXiv preprint arXiv:1704.03471\/} (2017).

\bibitem{bengio2009curriculum}
{\sc Bengio, Y., Louradour, J., Collobert, R., and Weston, J.}
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning\/} (2009), pp.~41--48.

\bibitem{brown2020language}
{\sc Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.}
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems 33\/} (2020),
  1877--1901.

\bibitem{conneau2018you}
{\sc Conneau, A., Kruszewski, G., Lample, G., Barrault, L., and Baroni, M.}
\newblock What you can cram into a single \$\&!\#* vector: Probing sentence
  embeddings for linguistic properties.
\newblock {\em arXiv preprint arXiv:1805.01070\/} (2018).

\bibitem{elhage2022toy}
{\sc Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S.,
  Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et~al.}
\newblock Toy models of superposition.
\newblock {\em arXiv preprint arXiv:2209.10652\/} (2022).

\bibitem{furlanello2018born}
{\sc Furlanello, T., Lipton, Z., Tschannen, M., Itti, L., and Anandkumar, A.}
\newblock Born again neural networks.
\newblock {\em International Conference on Machine Learning\/} (2018),
  1607--1616.

\bibitem{gal2016dropout}
{\sc Gal, Y., and Ghahramani, Z.}
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em International conference on machine learning\/} (2016), PMLR,
  pp.~1050--1059.

\bibitem{guo2017calibration}
{\sc Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.}
\newblock On calibration of modern neural networks.
\newblock In {\em International conference on machine learning\/} (2017), PMLR,
  pp.~1321--1330.

\bibitem{ho2023large}
{\sc Ho, N., Schmid, L., and Yun, S.-Y.}
\newblock Large language models are reasoning teachers.
\newblock {\em arXiv preprint arXiv:2212.10071\/} (2023).

\bibitem{houlsby2019parameter}
{\sc Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe,
  Q., Gesmundo, A., Attariyan, M., and Gelly, S.}
\newblock Parameter-efficient transfer learning for nlp.
\newblock In {\em International Conference on Machine Learning\/} (2019), PMLR,
  pp.~2790--2799.

\bibitem{hu2022lora}
{\sc Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
  L., and Chen, W.}
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685\/} (2022).

\bibitem{jawahar2019does}
{\sc Jawahar, G., Sagot, B., and Seddah, D.}
\newblock What does bert learn about the structure of language?
\newblock {\em ACL 2019\/} (2019).

\bibitem{ji2023survey}
{\sc Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang,
  Y.~J., Madotto, A., and Fung, P.}
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Computing Surveys 55}, 12 (2023), 1--38.

\bibitem{jiang2023mistral}
{\sc Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S.,
  Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.}
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825\/} (2023).

\bibitem{jiang2021can}
{\sc Jiang, Z., Araki, J., Ding, H., and Neubig, G.}
\newblock How can we know when language models know? on the calibration of
  language models for question answering.
\newblock {\em Transactions of the Association for Computational Linguistics
  9\/} (2021), 962--977.

\bibitem{kadavath2022language}
{\sc Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E.,
  Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., et~al.}
\newblock Language models (mostly) know what they know.
\newblock {\em arXiv preprint arXiv:2207.05221\/} (2022).

\bibitem{koh2017understanding}
{\sc Koh, P.~W., and Liang, P.}
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International conference on machine learning\/} (2017), PMLR,
  pp.~1885--1894.

\bibitem{kuhn2023semantic}
{\sc Kuhn, L., Gal, Y., and Farquhar, S.}
\newblock Semantic uncertainty: Linguistic invariances for uncertainty
  estimation in natural language generation.
\newblock {\em arXiv preprint arXiv:2302.09664\/} (2023).

\bibitem{lakshminarayanan2017simple}
{\sc Lakshminarayanan, B., Pritzel, A., and Blundell, C.}
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock {\em Advances in neural information processing systems 30\/} (2017).

\bibitem{li2024inference}
{\sc Li, K., Patel, O., Vi{\'e}gas, F., Pfister, H., and Wattenberg, M.}
\newblock Inference-time intervention: Eliciting truthful answers from a
  language model.
\newblock {\em Advances in Neural Information Processing Systems 36\/} (2024).

\bibitem{libovicky2020language}
{\sc Libovick{\'y}, J., Rosa, R., and Fraser, A.}
\newblock Language-neutral bert and what it understands.
\newblock {\em arXiv preprint arXiv:2004.02070\/} (2020).

\bibitem{lin2022teaching}
{\sc Lin, S., Hilton, J., and Evans, O.}
\newblock Teaching models to express their uncertainty in words.
\newblock {\em arXiv preprint arXiv:2205.14334\/} (2022).

\bibitem{magister2023teaching}
{\sc Magister, L.~C., Mallinson, J., Adamek, J., Malmi, E., and Severyn, A.}
\newblock Teaching small language models to reason.
\newblock {\em arXiv preprint arXiv:2212.08410\/} (2023).

\bibitem{malinin2021uncertainty}
{\sc Malinin, A., and Gales, M.}
\newblock Uncertainty estimation in autoregressive structured prediction.
\newblock {\em arXiv preprint arXiv:2002.07650\/} (2021).

\bibitem{mikolov2013linguistic}
{\sc Mikolov, T., Yih, W.-t., and Zweig, G.}
\newblock Linguistic regularities in continuous space word representations.
\newblock {\em Proceedings of NAACL-HLT\/} (2013), 746--751.

\bibitem{olah2017feature}
{\sc Olah, C., Mordvintsev, A., and Schubert, L.}
\newblock Feature visualization.
\newblock {\em Distill 2}, 11 (2017), e7.

\bibitem{openai2023gpt4}
{\sc OpenAI}.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774\/} (2023).

\bibitem{park2023linear}
{\sc Park, K., Choe, Y.~J., and Veitch, V.}
\newblock The linear representation hypothesis and the geometry of large
  language models.
\newblock {\em arXiv preprint arXiv:2311.03658\/} (2023).

\bibitem{paszke2019pytorch}
{\sc Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,
  Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et~al.}
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems 32\/} (2019).

\bibitem{pfeiffer2020adapterhub}
{\sc Pfeiffer, J., R{\"u}ckl{\'e}, A., Poth, C., Kamath, A., Vuli{\'c}, I.,
  Ruder, S., Cho, K., and Gurevych, I.}
\newblock Adapterhub: A framework for adapting transformers.
\newblock {\em arXiv preprint arXiv:2007.07779\/} (2020).

\bibitem{pires2019multilingual}
{\sc Pires, T., Schlinger, E., and Garrette, D.}
\newblock How multilingual is multilingual bert?
\newblock {\em arXiv preprint arXiv:1906.01502\/} (2019).

\bibitem{post2018fast}
{\sc Post, M.}
\newblock A call for clarity in reporting bleu scores.
\newblock {\em arXiv preprint arXiv:1804.08771\/} (2018).

\bibitem{settles2009active}
{\sc Settles, B.}
\newblock Active learning literature survey.
\newblock {\em University of Wisconsin-Madison Department of Computer
  Sciences\/} (2009).

\bibitem{touvron2023llama}
{\sc Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.}
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288\/} (2023).

\bibitem{turner2023activation}
{\sc Turner, A.~M., Thiergart, L., Udell, D., Leech, G., Mini, U., and
  MacDiarmid, M.}
\newblock Activation addition: Steering language models without optimization.
\newblock {\em arXiv preprint arXiv:2308.10248\/} (2023).

\bibitem{wang2024mmlu}
{\sc Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W.,
  Arulraj, A., He, X., Jiang, Z., et~al.}
\newblock Mmlu-pro: A more robust and challenging multi-task language
  understanding benchmark.
\newblock {\em arXiv preprint arXiv:2406.01574\/} (2024).

\bibitem{wendler2024llamas}
{\sc Wendler, C., Veith, V., Haim, N., Kapur, R., Hassid, M., Pietrek, M., and
  Eichler, N.}
\newblock Do llamas work in english? on the latent language of multilingual
  transformers.
\newblock {\em arXiv preprint arXiv:2402.10588\/} (2024).

\bibitem{winata2021language}
{\sc Winata, G.~I., Madotto, A., Lin, Z., Liu, R., Yosinski, J., and Fung, P.}
\newblock Language models are few-shot multilingual learners.
\newblock {\em arXiv preprint arXiv:2109.07684\/} (2021).

\bibitem{wolf2020transformers}
{\sc Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A.,
  Cistac, P., Rault, T., Louf, R., Funtowicz, M., et~al.}
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 conference on empirical methods in
  natural language processing: system demonstrations\/} (2020), pp.~38--45.

\bibitem{wu2019beto}
{\sc Wu, S., and Dredze, M.}
\newblock Beto, bentz, becas: The surprising cross-lingual effectiveness of
  bert.
\newblock {\em arXiv preprint arXiv:1904.09077\/} (2019).

\bibitem{xie2020self}
{\sc Xie, Q., Luong, M.-T., Hovy, E., and Le, Q.~V.}
\newblock Self-training with noisy student improves imagenet classification.
\newblock {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition\/} (2020), 10687--10698.

\bibitem{qwen2024}
{\sc Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C.,
  Liu, D., Huang, F., et~al.}
\newblock Qwen2 technical report.
\newblock {\em arXiv preprint arXiv:2407.10671\/} (2024).

\end{thebibliography}
