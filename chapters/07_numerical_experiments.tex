\chapter{Numerical experiments}

This chapter presents the experimental setup and results for each of the three studies comprising this thesis.

\section{Study 1: Uncertainty Estimation via Token-Wise Entropy}

\subsection{Experimental Setup}

\textbf{Dataset}: We use the MMLU-Pro benchmark~\cite{wang2024mmlu}, which contains challenging multiple-choice questions across 14 diverse domains including Biology, Chemistry, Computer Science, Economics, Engineering, Health, History, Law, Math, Philosophy, Physics, Psychology, Business, and Other.

\textbf{Models}: We evaluate several large language models:
\begin{itemize}
    \item Llama-3-8B and Llama-3-70B~\cite{touvron2023llama}
    \item Qwen-2.5 family (1.5B, 3B, 7B, 14B, 32B, 72B)~\cite{qwen2024}
    \item Mistral-7B~\cite{jiang2023mistral}
\end{itemize}

\textbf{Metrics}: We measure the predictive power of entropy features using ROC-AUC for distinguishing correct from incorrect answers.

\subsection{Results}

Table~\ref{tab:entropy_results} presents the ROC-AUC scores for predicting incorrect answers using the max-to-min entropy ratio feature across different domains.

\begin{table}[!htb]
    \centering
    \caption{ROC-AUC scores for predicting incorrect answers using max-to-min entropy ratio on MMLU-Pro. Higher values indicate better discrimination between correct and incorrect answers.}
    \begin{tabular}{lccc}
    \toprule
    Domain & Llama-3-8B & Qwen-2.5-7B & Qwen-2.5-72B \\
    \midrule
    Biology & 0.78 & 0.81 & $\mathbf{0.83}$ \\
    Chemistry & 0.72 & 0.75 & 0.79 \\
    Computer Science & 0.69 & 0.73 & 0.77 \\
    Economics & 0.71 & 0.74 & 0.78 \\
    Math & 0.65 & 0.68 & 0.72 \\
    Physics & 0.67 & 0.71 & 0.76 \\
    \midrule
    Average & 0.70 & 0.74 & 0.78 \\
    \bottomrule
    \end{tabular}
    \label{tab:entropy_results}
\end{table}

Key findings:
\begin{enumerate}
    \item The max-to-min entropy ratio consistently outperforms other entropy features (mean, max, min, standard deviation) across all domains.
    \item Larger models show better calibration between entropy patterns and correctness.
    \item Domain-specific performance varies, with Biology showing the highest ROC-AUC (0.83) and Math showing the lowest (0.72).
    \item The Model-as-Judge approach provides complementary signal that can be combined with entropy features for improved prediction.
\end{enumerate}

\subsection{Comparison with Baselines}

We compare our entropy-based approach with baseline uncertainty measures:

\begin{table}[!htb]
    \centering
    \caption{Comparison of uncertainty estimation methods (average ROC-AUC across domains).}
    \begin{tabular}{lc}
    \toprule
    Method & ROC-AUC \\
    \midrule
    First-token probability & 0.62 \\
    Mean sequence probability & 0.65 \\
    Verbalized confidence & 0.68 \\
    Mean entropy & 0.71 \\
    Max-to-min entropy ratio (ours) & $\mathbf{0.78}$ \\
    \bottomrule
    \end{tabular}
    \label{tab:baseline_comparison}
\end{table}

\section{Study 2: Language Steering in Latent Space}

\subsection{Experimental Setup}

\textbf{Models}: We evaluate multilingual models:
\begin{itemize}
    \item Llama-3-8B-Instruct
    \item Qwen-2.5-7B-Instruct
    \item Mistral-7B-Instruct-v0.3
\end{itemize}

\textbf{Languages}: We focus on English-Russian language pairs, as code-switching between these languages is common in multilingual deployments.

\textbf{Dataset}: We collect hidden states from 1,000 text samples per language from Wikipedia and news articles.

\textbf{Metrics}:
\begin{itemize}
    \item Classification accuracy for language direction identification
    \item KL divergence reduction for code-switching mitigation
    \item Perplexity change to measure generation quality
\end{itemize}

\subsection{Language Direction Identification}

Table~\ref{tab:lang_classification} shows the classification accuracy for identifying language from hidden state projections.

\begin{table}[!htb]
    \centering
    \caption{Language classification accuracy using PCA-identified directions at different layers.}
    \begin{tabular}{lccc}
    \toprule
    Model & Layer 8 & Layer 16 & Layer 24 \\
    \midrule
    Llama-3-8B & 0.92 & $\mathbf{0.98}$ & 0.96 \\
    Qwen-2.5-7B & 0.94 & $\mathbf{0.99}$ & 0.97 \\
    Mistral-7B & 0.91 & $\mathbf{0.97}$ & 0.95 \\
    \bottomrule
    \end{tabular}
    \label{tab:lang_classification}
\end{table}

Results show that:
\begin{enumerate}
    \item Language directions can be reliably identified with 95--99\% accuracy.
    \item Middle layers (around layer 16 for 32-layer models) provide the best separation.
    \item The first principal component captures the majority of language-related variance.
\end{enumerate}

\subsection{Code-Switching Reduction}

We evaluate steering effectiveness by prompting models in Russian and measuring the fraction of non-Russian tokens in the response.

\begin{table}[!htb]
    \centering
    \caption{Code-switching reduction through activation steering. KL divergence measures distance from target language distribution (lower is better).}
    \begin{tabular}{lccc}
    \toprule
    Model & Baseline KL & Steered KL & Reduction \\
    \midrule
    Llama-3-8B & 0.42 & 0.21 & 50\% \\
    Qwen-2.5-7B & 0.38 & 0.17 & $\mathbf{55\%}$ \\
    Mistral-7B & 0.45 & 0.24 & 47\% \\
    \bottomrule
    \end{tabular}
    \label{tab:steering_results}
\end{table}

Steering reduces code-switching by 47--55\% without significant degradation in generation quality (perplexity increase $<5\%$).

\section{Study 3: Complexity-Aware Fine-Tuning}

\subsection{Experimental Setup}

\textbf{Dataset}: MMLU-Pro training split, containing approximately 12,000 examples across 14 domains.

\textbf{Models}:
\begin{itemize}
    \item Student models: Qwen-2.5-1.5B, Qwen-2.5-3B
    \item Teacher model: Qwen-2.5-72B-Instruct
\end{itemize}

\textbf{Baselines}:
\begin{itemize}
    \item Full SFT: Train on all examples with direct supervision
    \item Random subset: Train on randomly selected subset matching our data budget
    \item CoT distillation only: Apply chain-of-thought distillation to all examples
\end{itemize}

\textbf{Metrics}: Accuracy on MMLU-Pro test set.

\subsection{Main Results}

Table~\ref{tab:finetuning_results} presents the main fine-tuning results.

\begin{table}[!htb]
    \centering
    \caption{Accuracy comparison of fine-tuning approaches on MMLU-Pro test set.}
    \begin{tabular}{lcccc}
    \toprule
    Method & Data Used & Qwen-2.5-1.5B & Qwen-2.5-3B \\
    \midrule
    Base model & -- & 0.28 & 0.35 \\
    Full SFT & 100\% & 0.39 & 0.51 \\
    Random subset & 19\% & 0.34 & 0.44 \\
    CoT distill (all) & 100\% & 0.45 & 0.57 \\
    Complexity-aware (ours) & 19\% & $\mathbf{0.52}$ & $\mathbf{0.64}$ \\
    \bottomrule
    \end{tabular}
    \label{tab:finetuning_results}
\end{table}

Key findings:
\begin{enumerate}
    \item Our complexity-aware approach achieves the highest accuracy while using only 19\% of the training data.
    \item Compared to full SFT, we improve accuracy from 0.39 to 0.52 (+33\%) for Qwen-2.5-1.5B and from 0.51 to 0.64 (+25\%) for Qwen-2.5-3B.
    \item Even compared to full CoT distillation on all data, our targeted approach achieves higher accuracy with 81\% less data.
\end{enumerate}

\subsection{Ablation Study}

We ablate the key components of our approach:

\begin{table}[!htb]
    \centering
    \caption{Ablation study on Qwen-2.5-1.5B.}
    \begin{tabular}{lc}
    \toprule
    Configuration & Accuracy \\
    \midrule
    Full method & $\mathbf{0.52}$ \\
    Without stratification (CoT all) & 0.45 \\
    Without CoT (SFT all) & 0.39 \\
    Easy examples only (SFT) & 0.36 \\
    Hard examples only (CoT) & 0.48 \\
    \bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table}

The ablation confirms that both stratification and the combination of SFT (for easy examples) and CoT distillation (for hard examples) are necessary for optimal performance.

\subsection{Complexity Threshold Analysis}

We analyze the effect of the complexity threshold $\tau$ on performance:

\begin{table}[!htb]
    \centering
    \caption{Effect of complexity threshold (percentile of training data classified as ``hard'').}
    \begin{tabular}{lccc}
    \toprule
    Hard Fraction & Easy (SFT) & Hard (CoT) & Accuracy \\
    \midrule
    10\% & 90\% & 10\% & 0.47 \\
    20\% & 80\% & 20\% & $\mathbf{0.52}$ \\
    30\% & 70\% & 30\% & 0.51 \\
    50\% & 50\% & 50\% & 0.49 \\
    \bottomrule
    \end{tabular}
    \label{tab:threshold}
\end{table}

Optimal performance is achieved when approximately 20\% of examples are classified as hard and receive CoT distillation.

\section{Summary of Results}

Across all three studies, we demonstrate that signals from the model's forward pass provide valuable information for addressing key challenges in LLM deployment:

\begin{enumerate}
    \item \textbf{Uncertainty estimation}: Token-wise entropy patterns achieve ROC-AUC up to 0.83 for predicting answer correctness.
    \item \textbf{Language control}: PCA-identified language directions enable 47--55\% reduction in code-switching.
    \item \textbf{Training efficiency}: Complexity-aware fine-tuning improves accuracy while using 81\% less training data.
\end{enumerate}

These results validate our central hypothesis that accessible signals from forward passes encode sufficient information to improve reliability, control, and efficiency without requiring architectural modifications.
